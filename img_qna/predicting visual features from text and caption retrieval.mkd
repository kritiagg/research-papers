# [Predicting Visual Features from Text forImage and Video Caption Retrieval (2018)](https://arxiv.org/pdf/1709.01362.pdf)
- Creates Word2VisualVEc for vedio and caotion retrieval. The main idea being converting from text space to visual space.
- Uses Word2Vec and RNN (the output of the last RNN as the sentence embedding)
- transform the encoding into a higher-dimensional visual feature space via a multi-layer perceptron
- Previous works: 
  * Linear CCA
  * Using pretrained fc6 AlexNet as image features
  * Use LSTM for learning text and image modalities and then using compact bilinear pooling for interations
 - Model:
 ![model.PNG](../img/img_in_qna/img1.PNG)
 - For text encoding:
  * Bag Of Words: avg pool at the end to get the embedding of the complete sentence
  * Word2Vec on Flicker tags
  * RNN
  * All three representations are contatenated
  - For image encoding:
   * Use GoogleNet/ResNet-152 features



# [Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics by Hodosh et al. (2015)](https://www.ijcai.org/Proceedings/15/Papers/593.pdf)
- Changed the problem of captioning the images to a ranking task. By ranking the captions given an image. 
- This technique helps in getting rid of techniques like BLUE score and ROUGE score as metrics.
- Used KCCA (Kernal Canonical Correlation Analysis) to bring the text and image in same latent space.
- Used a subsequence string Kernel for Text kernal and spatial pyramid kernel for images.


# [EZLearn: Exploiting Organic Supervision in Automated Data Annotation]
(https://arxiv.org/pdf/1709.08600.pdf)
- Input: images, their textual descriptions which can help in identifying the class.
- Output: Given an input image, classify the image in the specified class. So, basically cluster the images using the textual and image features.
- Dataset: medical dataset containing the images for genes etc.
- Method:
 * Use distant supervision or unsupervised learning in classifying images into a class
 * 2 classifiers: main classifier: image-> class. image features using denoising auto encoder and then multinomial logistic regression
   * aux classifier: text description to class. -> fast text
 * The 2 classifiers use the output of each other as the labeleed set. For teh beginning the main classifier treat the presence of class words in the text descriptions as the label.
   

